# Type I: Context-memory conflict

- Merging Generated and Retrieved Knowledge for Open-Domain QA, [[Paper](https://arxiv.org/abs/2310.14393)]
- Taming Knowledge Conflicts in Language Models, Li et al, arxiv, https://arxiv.org/pdf/2503.10996
- Improving Factual Consistency for Knowledge-Grounded Dialogue Systems via Knowledge Enhancement and Alignment, *Xue et al.*, **EMNLP 2023**, [[Paper](https://aclanthology.org/2023.findings-emnlp.525.pdf)]
- TrueTeacher: Learning Factual Consistency Evaluation with Large Language Models, *Gekhman et al.*, **EMNLP 2023**, [[Paper](https://aclanthology.org/2023.emnlp-main.127.pdf)]
- Dynamic Attention-Guided Context Decoding for Mitigating Context Faithfulness Hallucinations in Large Language Models, Huang et al, arxiv, https://arxiv.org/pdf/2501.01059
- Defending Against Disinformation Attacks in Open-Domain Question Answering, [[Paper](https://arxiv.org/abs/2212.10002)]
- Context-DPO: Aligning Language Models for Context-Faithfulness, Bi et al, arxiv, https://arxiv.org/abs/2412.15280
- Improving Temporal Generalization of Pre-trained Language Models with Lexical Semantic Change, *Su et al.*, **EMNLP 2022**, [[Paper](https://aclanthology.org/2022.emnlp-main.428/)]
- In-context Pretraining: Language Modeling Beyond Document Boundaries, [[Paper](https://arxiv.org/abs/2310.10638)]
- Context-faithful Prompting for Large Language Models, *Zhou et al.*, **EMNLP 2023**, [[Paper](https://aclanthology.org/2023.findings-emnlp.968.pdf)]
- Findings of the Association for Computational Linguistics: **ACL 2022**, [[Paper](https://aclanthology.org/volumes/2022.findings-acl/)]
- Trusting Your Evidence: Hallucinate Less with Context-aware Decoding, *Shi et al.*, **NAACL 2024**, [[Paper](https://aclanthology.org/2024.naacl-short.69.pdf)]
- KnowPO: Knowledge-aware Preference Optimization for Controllable Knowledge Selection in Retrieval-Augmented Language Models, Zhang et al,  AAAI 2025, https://arxiv.org/abs/2408.032971. Large Language Models with Controllable Working Memory, *Li et al.*, **ACL 2023**, [[Paper](https://aclanthology.org/2023.findings-acl.112.pdf)]
- Discern and answer: Mitigating the impact of misinformation in retrieval-augmented models with discriminators, [[Paper](https://arxiv.org/abs/2305.01579)]
- Tug-of-war between knowledge: Exploring and resolving knowledge conflicts in retrieval-augmented language models, **COLING 2024**, [[Paper](https://arxiv.org/abs/2402.14409)]
- Cutting Off the Head Ends the Conflict: A Mechanism for Interpreting and Mitigating Knowledge Conflicts in Language Models, **arXiv 2024**, [[Paper](https://arxiv.org/abs/2402.18154)]
- Tug-of-War Between Knowledge: Exploring and Resolving Knowledge Conflicts in Retrieval-Augmented Language Models,  [[Paper](https://arxiv.org/abs/2402.14409)]
- Resolving Knowledge Conflicts in Large Language Models,  [[Paper](https://arxiv.org/abs/2310.00935)]
- Characterizing mechanisms for factual recall in language models, **EMNLP 2023**, [[Paper](https://arxiv.org/abs/2310.15910)]
- On the Risk of Misinformation Pollution with Large Language Models, [[Paper](https://arxiv.org/abs/2305.13661)]
- PIP-KAG: Mitigating Knowledge Conflicts in Knowledge-Augmented Generation via Parametric Pruning, Huang et al, arxiv, https://arxiv.org/pdf/2502.15543
- Mitigating Temporal Misalignment by Discarding Outdated Facts, [[Paper](https://arxiv.org/abs/2305.14824)]
- AdaCAD: Adaptively Decoding to Balance Conflicts between Contextual and Parametric Knowledge, Wang et al, arxiv, https://arxiv.org/pdf/2409.07394
- DisentQA: Disentangling Parametric and Contextual Knowledge with Counterfactual Question Answering,  [[Paper](https://arxiv.org/abs/2211.05655)]
- Contrastive Decoding: Open-ended Text Generation as Optimization, *Li et al.*, **ACL 2023**, [[Paper](https://aclanthology.org/2023.acl-long.687.pdf)]
- The Earth is Flat because...: Investigating LLMs' Belief towards Misinformation via Persuasive Conversation, [[Paper](https://arxiv.org/abs/2312.09085)]
- Trustworthy Alignment of Retrieval-Augmented Large Language Models via Reinforcement Learning, Zhang et al, ICML 2024, https://arxiv.org/pdf/2410.16843






# Type II: Inter-context conflict

- A Linguistic Investigation of Machine Learning based Contradiction Detection Models: An Empirical Analysis and Future Perspectives.[[Paper](https://arxiv.org/abs/2210.10434)]
- Open Domain Question Answering with Conflicting Contexts, Liu et al, arxiv, https://arxiv.org/pdf/2410.12311
- Detecting Misinformation with LLM-Predicted Credibility Signals and Weak Supervision, *Leite et al.*, **arXiv 2023**, [[Paper](https://arxiv.org/pdf/2309.07601)]
- In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval. [[Paper](https://dl.acm.org/doi/proceedings/10.1145/3477495)]
- Discern and answer: Mitigating the impact of misinformation in retrieval-augmented models with discriminators, [[Paper](https://arxiv.org/abs/2305.01579)]
- Defending Against Disinformation Attacks in Open-Domain Question Answering, [[Paper](https://arxiv.org/abs/2212.10002)]
- Toward Robust RALMs: Revealing the Impact of Imperfect Retrieval on Retrieval-Augmented Language Models, Park et al, TACL 2024, https://arxiv.org/pdf/2410.15107
- Why So Gullible? Enhancing the Robustness of Retrieval-Augmented Models against Counterfactual Noise, Hong et al, NAACL 2024, https://arxiv.org/pdf/2305.01579
- What External Knowledge is Preferred by LLMs? Characterizing and Exploring Chain of Evidence in Imperfect Context, Chang et al, arxiv, https://arxiv.org/pdf/2412.12632
- Topological analysis of contradictions in text, *Wu et al.*, **SIGIR 2022**, [[Paper](https://dl.acm.org/doi/pdf/10.1145/3477495.3531881)]
- Retrieval-Augmented Generation with Conflicting Evidence, Wang et al, arxiv. https://arxiv.org/pdf/2504.13079
- Why So Gullible? Enhancing the Robustness of Retrieval-Augmented Models against Counterfactual Noise, *Hong et al.*, **arXiv 2024**, [[Paper](https://arxiv.org/pdf/2305.01579)]
- FACTOOL: Factuality Detection in Generative AI-A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios, *Chern et al.*, **arXiv 2023**, [[Paper](https://arxiv.org/pdf/2307.13528)]
- WikiContradiction: Detecting Self-Contradiction Articles on Wikipedia, *Hsu et al.*, **IEEE Big Data 2021**, [[Paper](https://www.computer.org/csdl/proceedings-article/big-data/2021/09671319/1A8hbIXOCPK)]
- FactLLaMA: Optimizing Instruction-Following Language Models with External Knowledge for Automated Fact-Checking. [[Paper](https://arxiv.org/abs/2309.00240)]
- Defending Against Disinformation Attacks in Open-Domain Question Answering, *Weller et al.*, **EACL 2024**, [[Paper](https://aclanthology.org/2024.eacl-short.35.pdf)]


# Type III: Intra-memory conflict

- Measuring and improving consistency in pretrained language models, *Elazar et al.*, **TACL 2021**, [[Paper](https://aclanthology.org/2021.tacl-1.60.pdf)]
- Benchmarking and improving generator-validator consistency of language models, *Li et al.*, **ICLR 2024**, [[Paper](https://openreview.net/pdf?id=phBS6YpTzC)]
- Improving language models meaning understanding and consistency by learning conceptual roles from dictionary, *Jang et al.*, **EMNLP 2023**, [[Paper](https://aclanthology.org/2023.emnlp-main.527.pdf)]
- Enhancing selfconsistency and performance of pre-trained language models through natural language inference, *Mitchell et al.*, **EMNLP 2022**, [[Paper](https://aclanthology.org/2022.emnlp-main.115.pdf)]
- Knowing what llms do not know: A simple yet effective self-detection method, *Zhao et al.*, **NAACL 2024**, [[Paper](https://aclanthology.org/2024.naacl-long.390.pdf)]
- Decoding by contrasting layers improves factuality in large language models, *Chuang et al.*, **ICLR 2024**, [[Paper](https://arxiv.org/pdf/2309.03883)]
- Inferencetime intervention: Eliciting truthful answers from a language model, *Li et al.*, **NeurIPS 2023**, [[Paper](https://arxiv.org/pdf/2306.03341)]



