# Type I: Context-memory conflict

1. Large Language Models with Controllable Working Memory, *Li et al.*, **ACL 2023**, [[Paper](https://aclanthology.org/2023.findings-acl.112.pdf)]
2. TrueTeacher: Learning Factual Consistency Evaluation with Large Language Models, *Gekhman et al.*, **EMNLP 2023**, [[Paper](https://aclanthology.org/2023.emnlp-main.127.pdf)]
3. Improving Factual Consistency for Knowledge-Grounded Dialogue Systems via Knowledge Enhancement and Alignment, *Xue et al.*, **EMNLP 2023**, [[Paper](https://aclanthology.org/2023.findings-emnlp.525.pdf)]
4. Improving Temporal Generalization of Pre-trained Language Models with Lexical Semantic Change, *Su et al.*, **EMNLP 2022**, [[Paper](https://aclanthology.org/2022.emnlp-main.428/)]
5. Context-faithful Prompting for Large Language Models, *Zhou et al.*, **EMNLP 2023**, [[Paper](https://aclanthology.org/2023.findings-emnlp.968.pdf)]
6. Trusting Your Evidence: Hallucinate Less with Context-aware Decoding, *Shi et al.*, **NAACL 2024**, [[Paper](https://aclanthology.org/2024.naacl-short.69.pdf)]
7. Contrastive Decoding: Open-ended Text Generation as Optimization, *Li et al.*, **ACL 2023**, [[Paper](https://aclanthology.org/2023.acl-long.687.pdf)]
8. Tug-of-war between knowledge: Exploring and resolving knowledge conflicts in retrieval-augmented language models, **COLING 2024**, [[Paper](https://arxiv.org/abs/2402.14409)]
9. Characterizing mechanisms for factual recall in language models, **EMNLP 2023**, [[Paper](https://arxiv.org/abs/2310.15910)]
10. Cutting Off the Head Ends the Conflict: A Mechanism for Interpreting and Mitigating Knowledge Conflicts in Language Models, **arXiv 2024**, [[Paper](https://arxiv.org/abs/2402.18154)]
11. Findings of the Association for Computational Linguistics: **ACL 2022**, [[Paper](https://aclanthology.org/volumes/2022.findings-acl/)]
12. In-context Pretraining: Language Modeling Beyond Document Boundaries, [[Paper](https://arxiv.org/abs/2310.10638)]
13. Mitigating Temporal Misalignment by Discarding Outdated Facts, [[Paper](https://arxiv.org/abs/2305.14824)]
14. On the Risk of Misinformation Pollution with Large Language Models, [[Paper](https://arxiv.org/abs/2305.13661)]
15. The Earth is Flat because...: Investigating LLMs' Belief towards Misinformation via Persuasive Conversation, [[Paper](https://arxiv.org/abs/2312.09085)]
16. Defending Against Disinformation Attacks in Open-Domain Question Answering, [[Paper](https://arxiv.org/abs/2212.10002)]
17. Discern and answer: Mitigating the impact of misinformation in retrieval-augmented models with discriminators, [[Paper](https://arxiv.org/abs/2305.01579)]
18. DisentQA: Disentangling Parametric and Contextual Knowledge with Counterfactual Question Answering,  [[Paper](https://arxiv.org/abs/2211.05655)]
19. Resolving Knowledge Conflicts in Large Language Models,  [[Paper](https://arxiv.org/abs/2310.00935)]
20. Merging Generated and Retrieved Knowledge for Open-Domain QA, [[Paper](https://arxiv.org/abs/2310.14393)]
21. Tug-of-War Between Knowledge: Exploring and Resolving Knowledge Conflicts in Retrieval-Augmented Language Models,  [[Paper](https://arxiv.org/abs/2402.14409)]





# Type II: Inter-context conflict

1. WikiContradiction: Detecting Self-Contradiction Articles on Wikipedia, *Hsu et al.*, **IEEE Big Data 2021**, [[Paper](https://www.computer.org/csdl/proceedings-article/big-data/2021/09671319/1A8hbIXOCPK)]
2. Topological analysis of contradictions in text, *Wu et al.*, **SIGIR 2022**, [[Paper](https://dl.acm.org/doi/pdf/10.1145/3477495.3531881)]
3. FACTOOL: Factuality Detection in Generative AI-A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios, *Chern et al.*, **arXiv 2023**, [[Paper](https://arxiv.org/pdf/2307.13528)]
4. Detecting Misinformation with LLM-Predicted Credibility Signals and Weak Supervision, *Leite et al.*, **arXiv 2023**, [[Paper](https://arxiv.org/pdf/2309.07601)]
5. Why So Gullible? Enhancing the Robustness of Retrieval-Augmented Models against Counterfactual Noise, *Hong et al.*, **arXiv 2024**, [[Paper](https://arxiv.org/pdf/2305.01579)]
6. Defending Against Disinformation Attacks in Open-Domain Question Answering, *Weller et al.*, **EACL 2024**, [[Paper](https://aclanthology.org/2024.eacl-short.35.pdf)]
7. A Linguistic Investigation of Machine Learning based Contradiction Detection Models: An Empirical Analysis and Future Perspectives.[[Paper](https://arxiv.org/abs/2210.10434)]
8. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval. [[Paper](https://dl.acm.org/doi/proceedings/10.1145/3477495)]
9. FactLLaMA: Optimizing Instruction-Following Language Models with External Knowledge for Automated Fact-Checking. [[Paper](https://arxiv.org/abs/2309.00240)]
10. Defending Against Disinformation Attacks in Open-Domain Question Answering, [[Paper](https://arxiv.org/abs/2212.10002)]
11. Discern and answer: Mitigating the impact of misinformation in retrieval-augmented models with discriminators, [[Paper](https://arxiv.org/abs/2305.01579)]



# Type III: Intra-memory conflict

1. Measuring and improving consistency in pretrained language models, *Elazar et al.*, **TACL 2021**, [[Paper](https://aclanthology.org/2021.tacl-1.60.pdf)]
2. Benchmarking and improving generator-validator consistency of language models, *Li et al.*, **ICLR 2024**, [[Paper](https://openreview.net/pdf?id=phBS6YpTzC)]
3. Improving language models meaning understanding and consistency by learning conceptual roles from dictionary, *Jang et al.*, **EMNLP 2023**, [[Paper](https://aclanthology.org/2023.emnlp-main.527.pdf)]
4. Enhancing selfconsistency and performance of pre-trained language models through natural language inference, *Mitchell et al.*, **EMNLP 2022**, [[Paper](https://aclanthology.org/2022.emnlp-main.115.pdf)]
5. Knowing what llms do not know: A simple yet effective self-detection method, *Zhao et al.*, **NAACL 2024**, [[Paper](https://aclanthology.org/2024.naacl-long.390.pdf)]
6. Decoding by contrasting layers improves factuality in large language models, *Chuang et al.*, **ICLR 2024**, [[Paper](https://arxiv.org/pdf/2309.03883)]
7. Inferencetime intervention: Eliciting truthful answers from a language model, *Li et al.*, **NeurIPS 2023**, [[Paper](https://arxiv.org/pdf/2306.03341)]



